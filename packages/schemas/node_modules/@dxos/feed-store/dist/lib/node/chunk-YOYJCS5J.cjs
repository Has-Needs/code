"use strict";
var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getProtoOf = Object.getPrototypeOf;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(
  // If the importer is in node compatibility mode or this is not an ESM
  // file that has been converted to a CommonJS file using a Babel-
  // compatible transform (i.e. "__esModule" has not been set), then set
  // "default" to the CommonJS "module.exports" for node compatibility.
  isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: true }) : target,
  mod
));
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);
var chunk_YOYJCS5J_exports = {};
__export(chunk_YOYJCS5J_exports, {
  FeedFactory: () => FeedFactory,
  FeedStore: () => FeedStore,
  FeedWrapper: () => FeedWrapper
});
module.exports = __toCommonJS(chunk_YOYJCS5J_exports);
var import_node_util = require("node:util");
var import_streamx = require("streamx");
var import_async = require("@dxos/async");
var import_debug = require("@dxos/debug");
var import_invariant = require("@dxos/invariant");
var import_log = require("@dxos/log");
var import_util = require("@dxos/util");
var import_lodash = __toESM(require("lodash.defaultsdeep"));
var import_crypto = require("@dxos/crypto");
var import_debug2 = require("@dxos/debug");
var import_hypercore = require("@dxos/hypercore");
var import_log2 = require("@dxos/log");
var import_async2 = require("@dxos/async");
var import_debug3 = require("@dxos/debug");
var import_invariant2 = require("@dxos/invariant");
var import_keys = require("@dxos/keys");
var import_log3 = require("@dxos/log");
var import_util2 = require("@dxos/util");
var __dxlog_file = "/home/runner/work/dxos/dxos/packages/common/feed-store/src/feed-wrapper.ts";
var FeedWrapper = class {
  constructor(_hypercore, _key, _storageDirectory) {
    this._hypercore = _hypercore;
    this._key = _key;
    this._storageDirectory = _storageDirectory;
    this._pendingWrites = /* @__PURE__ */ new Set();
    this._binder = (0, import_util.createBinder)(this._hypercore);
    this._writeLock = new import_async.Trigger();
    this._closed = false;
    this.on = this._binder.fn(this._hypercore.on);
    this.off = this._binder.fn(this._hypercore.off);
    this.open = this._binder.async(this._hypercore.open);
    this._close = this._binder.async(this._hypercore.close);
    this.close = async () => {
      if (this._pendingWrites.size) {
        import_log.log.warn("Closing feed with pending writes", {
          feed: this._key,
          count: this._pendingWrites.size,
          pendingWrites: Array.from(this._pendingWrites.values()).map((stack) => stack.getStack())
        }, {
          F: __dxlog_file,
          L: 173,
          S: this,
          C: (f, a) => f(...a)
        });
      }
      this._closed = true;
      await this.flushToDisk();
      await this._close();
    };
    this.has = this._binder.fn(this._hypercore.has);
    this.get = this._binder.async(this._hypercore.get);
    this.append = this._binder.async(this._hypercore.append);
    this.download = this._binder.fn(this._hypercore.download);
    this.undownload = this._binder.fn(this._hypercore.undownload);
    this.setDownloading = this._binder.fn(this._hypercore.setDownloading);
    this.replicate = this._binder.fn(this._hypercore.replicate);
    this.clear = this._binder.async(this._hypercore.clear);
    this.proof = this._binder.async(this._hypercore.proof);
    this.put = this._binder.async(this._hypercore.put);
    this.putBuffer = this._binder.async(this._hypercore._putBuffer);
    (0, import_invariant.invariant)(this._hypercore, void 0, {
      F: __dxlog_file,
      L: 37,
      S: this,
      A: [
        "this._hypercore",
        ""
      ]
    });
    (0, import_invariant.invariant)(this._key, void 0, {
      F: __dxlog_file,
      L: 38,
      S: this,
      A: [
        "this._key",
        ""
      ]
    });
    this._writeLock.wake();
  }
  [import_node_util.inspect.custom]() {
    return (0, import_debug.inspectObject)(this);
  }
  toJSON() {
    return {
      feedKey: this._key,
      length: this.properties.length,
      opened: this.properties.opened,
      closed: this.properties.closed
    };
  }
  get key() {
    return this._key;
  }
  get core() {
    return this._hypercore;
  }
  // TODO(burdon): Create proxy.
  get properties() {
    return this._hypercore;
  }
  createReadableStream(opts) {
    const self = this;
    const transform = new import_streamx.Transform({
      transform(data, cb) {
        void self._writeLock.wait().then(() => {
          this.push(data);
          cb();
        });
      }
    });
    const readStream = opts?.batch !== void 0 && opts?.batch > 1 ? new BatchedReadStream(this._hypercore, opts) : this._hypercore.createReadStream(opts);
    readStream.pipe(transform, (err) => {
    });
    return transform;
  }
  createFeedWriter() {
    return {
      write: async (data, { afterWrite } = {}) => {
        (0, import_log.log)("write", {
          feed: this._key,
          seq: this._hypercore.length
        }, {
          F: __dxlog_file,
          L: 97,
          S: this,
          C: (f, a) => f(...a)
        });
        (0, import_invariant.invariant)(!this._closed, "Feed closed", {
          F: __dxlog_file,
          L: 98,
          S: this,
          A: [
            "!this._closed",
            "'Feed closed'"
          ]
        });
        const stackTrace = new import_debug.StackTrace();
        try {
          this._pendingWrites.add(stackTrace);
          if (this._pendingWrites.size === 1) {
            this._writeLock.reset();
          }
          const receipt = await this.appendWithReceipt(data);
          await this.flushToDisk();
          await afterWrite?.(receipt);
          return receipt;
        } finally {
          this._pendingWrites.delete(stackTrace);
          if (this._pendingWrites.size === 0) {
            this._writeLock.wake();
          }
        }
      }
    };
  }
  async appendWithReceipt(data) {
    const seq = await this.append(data);
    (0, import_invariant.invariant)(seq < this.length, "Invalid seq after write", {
      F: __dxlog_file,
      L: 129,
      S: this,
      A: [
        "seq < this.length",
        "'Invalid seq after write'"
      ]
    });
    (0, import_log.log)("write complete", {
      feed: this._key,
      seq
    }, {
      F: __dxlog_file,
      L: 130,
      S: this,
      C: (f, a) => f(...a)
    });
    const receipt = {
      feedKey: this.key,
      seq
    };
    return receipt;
  }
  /**
  * Flush pending changes to disk.
  * Calling this is not required unless you want to explicitly wait for data to be written.
  */
  async flushToDisk() {
    await this._storageDirectory.flush();
  }
  get opened() {
    return this._hypercore.opened;
  }
  get closed() {
    return this._hypercore.closed;
  }
  get readable() {
    return this._hypercore.readable;
  }
  get length() {
    return this._hypercore.length;
  }
  get byteLength() {
    return this._hypercore.byteLength;
  }
  /**
  * Clear and check for integrity.
  */
  async safeClear(from, to) {
    (0, import_invariant.invariant)(from >= 0 && from < to && to <= this.length, "Invalid range", {
      F: __dxlog_file,
      L: 210,
      S: this,
      A: [
        "from >= 0 && from < to && to <= this.length",
        "'Invalid range'"
      ]
    });
    const CHECK_MESSAGES = 20;
    const checkBegin = to;
    const checkEnd = Math.min(checkBegin + CHECK_MESSAGES, this.length);
    const messagesBefore = await Promise.all((0, import_util.rangeFromTo)(checkBegin, checkEnd).map((idx) => this.get(idx, {
      valueEncoding: {
        decode: (x) => x
      }
    })));
    await this.clear(from, to);
    const messagesAfter = await Promise.all((0, import_util.rangeFromTo)(checkBegin, checkEnd).map((idx) => this.get(idx, {
      valueEncoding: {
        decode: (x) => x
      }
    })));
    for (let i = 0; i < messagesBefore.length; i++) {
      const before = (0, import_util.arrayToBuffer)(messagesBefore[i]);
      const after = (0, import_util.arrayToBuffer)(messagesAfter[i]);
      if (!before.equals(after)) {
        throw new Error("Feed corruption on clear. There has likely been a data loss.");
      }
    }
  }
};
var BatchedReadStream = class extends import_streamx.Readable {
  constructor(feed, opts = {}) {
    super({
      objectMode: true
    });
    this._reading = false;
    (0, import_invariant.invariant)(opts.live === true, "Only live mode supported", {
      F: __dxlog_file,
      L: 252,
      S: this,
      A: [
        "opts.live === true",
        "'Only live mode supported'"
      ]
    });
    (0, import_invariant.invariant)(opts.batch !== void 0 && opts.batch > 1, void 0, {
      F: __dxlog_file,
      L: 253,
      S: this,
      A: [
        "opts.batch !== undefined && opts.batch > 1",
        ""
      ]
    });
    this._feed = feed;
    this._batch = opts.batch;
    this._cursor = opts.start ?? 0;
  }
  _open(cb) {
    this._feed.ready(cb);
  }
  _read(cb) {
    if (this._reading) {
      return;
    }
    if (this._feed.bitfield.total(this._cursor, this._cursor + this._batch) === this._batch) {
      this._batchedRead(cb);
    } else {
      this._nonBatchedRead(cb);
    }
  }
  _nonBatchedRead(cb) {
    this._feed.get(this._cursor, {
      wait: true
    }, (err, data) => {
      if (err) {
        cb(err);
      } else {
        this._cursor++;
        this._reading = false;
        this.push(data);
        cb(null);
      }
    });
  }
  _batchedRead(cb) {
    this._feed.getBatch(this._cursor, this._cursor + this._batch, {
      wait: true
    }, (err, data) => {
      if (err) {
        cb(err);
      } else {
        this._cursor += data.length;
        this._reading = false;
        for (const item of data) {
          this.push(item);
        }
        cb(null);
      }
    });
  }
};
var __dxlog_file2 = "/home/runner/work/dxos/dxos/packages/common/feed-store/src/feed-factory.ts";
var FeedFactory = class {
  constructor({ root, signer, hypercore: hypercore2 }) {
    (0, import_log2.log)("FeedFactory", {
      options: hypercore2
    }, {
      F: __dxlog_file2,
      L: 43,
      S: this,
      C: (f, a) => f(...a)
    });
    this._root = root ?? (0, import_debug2.failUndefined)();
    this._signer = signer;
    this._hypercoreOptions = hypercore2;
  }
  get storageRoot() {
    return this._root;
  }
  async createFeed(publicKey, options) {
    if (options?.writable && !this._signer) {
      throw new Error("Signer required to create writable feeds.");
    }
    if (options?.secretKey) {
      import_log2.log.warn("Secret key ignored due to signer.", void 0, {
        F: __dxlog_file2,
        L: 58,
        S: this,
        C: (f, a) => f(...a)
      });
    }
    const key = await import_crypto.subtleCrypto.digest("SHA-256", Buffer.from(publicKey.toHex()));
    const opts = (0, import_lodash.default)({}, this._hypercoreOptions, {
      secretKey: this._signer && options?.writable ? Buffer.from("secret") : void 0,
      crypto: this._signer ? (0, import_hypercore.createCrypto)(this._signer, publicKey) : void 0,
      onwrite: options?.onwrite,
      noiseKeyPair: {}
    }, options);
    const storageDir = this._root.createDirectory(publicKey.toHex());
    const makeStorage = (filename) => {
      const { type, native } = storageDir.getOrCreateFile(filename);
      (0, import_log2.log)("created", {
        path: `${type}:${this._root.path}/${publicKey.truncate()}/${filename}`
      }, {
        F: __dxlog_file2,
        L: 82,
        S: this,
        C: (f, a) => f(...a)
      });
      return native;
    };
    const core = (0, import_hypercore.hypercore)(makeStorage, Buffer.from(key), opts);
    return new FeedWrapper(core, publicKey, storageDir);
  }
};
var __dxlog_file3 = "/home/runner/work/dxos/dxos/packages/common/feed-store/src/feed-store.ts";
var FeedStore = class {
  constructor({ factory }) {
    this._feeds = new import_util2.ComplexMap(import_keys.PublicKey.hash);
    this._mutexes = new import_util2.ComplexMap(import_keys.PublicKey.hash);
    this._closed = false;
    this.feedOpened = new import_async2.Event();
    this._factory = factory ?? (0, import_debug3.failUndefined)();
  }
  get size() {
    return this._feeds.size;
  }
  get feeds() {
    return Array.from(this._feeds.values());
  }
  /**
  * Get the open feed if it exists.
  */
  getFeed(publicKey) {
    return this._feeds.get(publicKey);
  }
  /**
  * Gets or opens a feed.
  * The feed is readonly unless a secret key is provided.
  */
  async openFeed(feedKey, { writable, sparse } = {}) {
    (0, import_log3.log)("opening feed", {
      feedKey
    }, {
      F: __dxlog_file3,
      L: 55,
      S: this,
      C: (f, a) => f(...a)
    });
    (0, import_invariant2.invariant)(feedKey, void 0, {
      F: __dxlog_file3,
      L: 56,
      S: this,
      A: [
        "feedKey",
        ""
      ]
    });
    (0, import_invariant2.invariant)(!this._closed, "Feed store is closed", {
      F: __dxlog_file3,
      L: 57,
      S: this,
      A: [
        "!this._closed",
        "'Feed store is closed'"
      ]
    });
    const mutex = (0, import_util2.defaultMap)(this._mutexes, feedKey, () => new import_async2.Mutex());
    return mutex.executeSynchronized(async () => {
      let feed = this.getFeed(feedKey);
      if (feed) {
        if (writable && !feed.properties.writable) {
          throw new Error(`Read-only feed is already open: ${feedKey.truncate()}`);
        } else if ((sparse ?? false) !== feed.properties.sparse) {
          throw new Error(`Feed already open with different sparse setting: ${feedKey.truncate()} [${sparse} !== ${feed.properties.sparse}]`);
        } else {
          await feed.open();
          return feed;
        }
      }
      feed = await this._factory.createFeed(feedKey, {
        writable,
        sparse
      });
      this._feeds.set(feed.key, feed);
      await feed.open();
      this.feedOpened.emit(feed);
      (0, import_log3.log)("opened", {
        feedKey
      }, {
        F: __dxlog_file3,
        L: 85,
        S: this,
        C: (f, a) => f(...a)
      });
      return feed;
    });
  }
  /**
  * Close all feeds.
  */
  async close() {
    (0, import_log3.log)("closing...", void 0, {
      F: __dxlog_file3,
      L: 94,
      S: this,
      C: (f, a) => f(...a)
    });
    this._closed = true;
    await Promise.all(Array.from(this._feeds.values()).map(async (feed) => {
      await feed.close();
      (0, import_invariant2.invariant)(feed.closed, void 0, {
        F: __dxlog_file3,
        L: 99,
        S: this,
        A: [
          "feed.closed",
          ""
        ]
      });
    }));
    this._feeds.clear();
    (0, import_log3.log)("closed", void 0, {
      F: __dxlog_file3,
      L: 108,
      S: this,
      C: (f, a) => f(...a)
    });
  }
};
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  FeedFactory,
  FeedStore,
  FeedWrapper
});
//# sourceMappingURL=chunk-YOYJCS5J.cjs.map
