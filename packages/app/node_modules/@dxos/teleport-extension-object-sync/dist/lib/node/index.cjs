"use strict";
var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getProtoOf = Object.getPrototypeOf;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(
  // If the importer is in node compatibility mode or this is not an ESM
  // file that has been converted to a CommonJS file using a Babel-
  // compatible transform (i.e. "__esModule" has not been set), then set
  // "default" to the CommonJS "module.exports" for node compatibility.
  isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: true }) : target,
  mod
));
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);
var node_exports = {};
__export(node_exports, {
  BlobStore: () => BlobStore,
  BlobSync: () => BlobSync,
  BlobSyncExtension: () => BlobSyncExtension,
  DEFAULT_CHUNK_SIZE: () => DEFAULT_CHUNK_SIZE
});
module.exports = __toCommonJS(node_exports);
var import_async = require("@dxos/async");
var import_context = require("@dxos/context");
var import_invariant = require("@dxos/invariant");
var import_log = require("@dxos/log");
var import_protocols = require("@dxos/protocols");
var import_proto = require("@dxos/protocols/proto");
var import_teleport = require("@dxos/teleport");
var import_util = require("@dxos/util");
var import_async2 = require("@dxos/async");
var import_context2 = require("@dxos/context");
var import_invariant2 = require("@dxos/invariant");
var import_keys = require("@dxos/keys");
var import_log2 = require("@dxos/log");
var import_blob = require("@dxos/protocols/proto/dxos/echo/blob");
var import_util2 = require("@dxos/util");
var import_node_path = __toESM(require("node:path"));
var import_async3 = require("@dxos/async");
var import_crypto = require("@dxos/crypto");
var import_invariant3 = require("@dxos/invariant");
var import_keys2 = require("@dxos/keys");
var import_proto2 = require("@dxos/protocols/proto");
var import_blob2 = require("@dxos/protocols/proto/dxos/echo/blob");
var import_util3 = require("@dxos/util");
function _ts_decorate(decorators, target, key, desc) {
  var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;
  if (typeof Reflect === "object" && typeof Reflect.decorate === "function") r = Reflect.decorate(decorators, target, key, desc);
  else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;
  return c > 3 && r && Object.defineProperty(target, key, r), r;
}
var __dxlog_file = "/home/runner/work/dxos/dxos/packages/core/mesh/teleport-extension-object-sync/src/blob-sync-extension.ts";
var MIN_WANT_LIST_UPDATE_INTERVAL = process.env.NODE_ENV === "test" ? 5 : 500;
var MAX_CONCURRENT_UPLOADS = 20;
var BlobSyncExtension = class extends import_teleport.RpcExtension {
  constructor(_params) {
    super({
      exposed: {
        BlobSyncService: import_proto.schema.getService("dxos.mesh.teleport.blobsync.BlobSyncService")
      },
      requested: {
        BlobSyncService: import_proto.schema.getService("dxos.mesh.teleport.blobsync.BlobSyncService")
      },
      timeout: 2e4,
      encodingOptions: {
        preserveAny: true
      }
    }), this._params = _params, this._ctx = new import_context.Context({
      onError: (err) => import_log.log.catch(err, void 0, {
        F: __dxlog_file,
        L: 35,
        S: this,
        C: (f, a) => f(...a)
      })
    }, {
      F: __dxlog_file,
      L: 35
    }), this._lastWantListUpdate = 0, this._localWantList = {
      blobs: []
    }, this._updateWantList = new import_async.DeferredTask(this._ctx, async () => {
      if (this._lastWantListUpdate + MIN_WANT_LIST_UPDATE_INTERVAL > Date.now()) {
        await (0, import_async.sleep)(this._lastWantListUpdate + MIN_WANT_LIST_UPDATE_INTERVAL - Date.now());
        if (this._ctx.disposed) {
          return;
        }
      }
      (0, import_log.log)("want", {
        list: this._localWantList
      }, {
        F: __dxlog_file,
        L: 49,
        S: this,
        C: (f, a) => f(...a)
      });
      await this.rpc.BlobSyncService.want(this._localWantList);
      this._lastWantListUpdate = Date.now();
    }), this._currentUploads = 0, this._upload = new import_async.DeferredTask(this._ctx, async () => {
      if (this._currentUploads >= MAX_CONCURRENT_UPLOADS) {
        return;
      }
      const blobChunks = await this._pickBlobChunks(MAX_CONCURRENT_UPLOADS - this._currentUploads);
      if (!blobChunks) {
        return;
      }
      for (const blobChunk of blobChunks) {
        if (this._ctx.disposed) {
          break;
        }
        this._currentUploads++;
        this.push(blobChunk).catch((err) => {
          if (err instanceof import_protocols.RpcClosedError) {
            return;
          }
          import_log.log.warn("push failed", {
            err
          }, {
            F: __dxlog_file,
            L: 76,
            S: this,
            C: (f, a) => f(...a)
          });
        }).finally(() => {
          this._currentUploads--;
          this.reconcileUploads();
        });
      }
    }), this.remoteWantList = {
      blobs: []
    };
  }
  async onOpen(context) {
    (0, import_log.log)("open", void 0, {
      F: __dxlog_file,
      L: 108,
      S: this,
      C: (f, a) => f(...a)
    });
    await super.onOpen(context);
    await this._params.onOpen();
  }
  async onClose(err) {
    (0, import_log.log)("close", void 0, {
      F: __dxlog_file,
      L: 114,
      S: this,
      C: (f, a) => f(...a)
    });
    await this._ctx.dispose();
    await this._params.onClose();
    await super.onClose(err);
  }
  async onAbort(err) {
    (0, import_log.log)("abort", void 0, {
      F: __dxlog_file,
      L: 121,
      S: this,
      C: (f, a) => f(...a)
    });
    await this._ctx.dispose();
    await this._params.onAbort();
    await super.onAbort(err);
  }
  async getHandlers() {
    return {
      BlobSyncService: {
        want: async (wantList) => {
          (0, import_log.log)("remote want", {
            remoteWantList: wantList
          }, {
            F: __dxlog_file,
            L: 131,
            S: this,
            C: (f, a) => f(...a)
          });
          this.remoteWantList = wantList;
          this.reconcileUploads();
        },
        push: async (data) => {
          (0, import_log.log)("received", {
            data
          }, {
            F: __dxlog_file,
            L: 136,
            S: this,
            C: (f, a) => f(...a)
          });
          await this._params.onPush(data);
        }
      }
    };
  }
  async push(data) {
    if (this._ctx.disposed) {
      return;
    }
    (0, import_log.log)("push", {
      data
    }, {
      F: __dxlog_file,
      L: 148,
      S: this,
      C: (f, a) => f(...a)
    });
    await this.rpc.BlobSyncService.push(data);
  }
  updateWantList(wantList) {
    if (this._ctx.disposed) {
      return;
    }
    this._localWantList = wantList;
    this._updateWantList.schedule();
  }
  reconcileUploads() {
    if (this._ctx.disposed) {
      return;
    }
    this._upload.schedule();
  }
  async _pickBlobChunks(amount = 1) {
    if (this._ctx.disposed) {
      return;
    }
    if (!this.remoteWantList.blobs || this.remoteWantList.blobs?.length === 0) {
      return;
    }
    const shuffled = [
      ...this.remoteWantList.blobs
    ].sort(() => Math.random() - 0.5);
    const chunks = [];
    for (const header of shuffled) {
      const meta = await this._params.blobStore.getMeta(header.id);
      if (!meta) {
        continue;
      }
      (0, import_invariant.invariant)(meta.bitfield, void 0, {
        F: __dxlog_file,
        L: 187,
        S: this,
        A: [
          "meta.bitfield",
          ""
        ]
      });
      (0, import_invariant.invariant)(meta.chunkSize, void 0, {
        F: __dxlog_file,
        L: 188,
        S: this,
        A: [
          "meta.chunkSize",
          ""
        ]
      });
      (0, import_invariant.invariant)(meta.length, void 0, {
        F: __dxlog_file,
        L: 189,
        S: this,
        A: [
          "meta.length",
          ""
        ]
      });
      if (header.chunkSize && header.chunkSize !== meta.chunkSize) {
        import_log.log.warn("Invalid chunk size", {
          header,
          meta
        }, {
          F: __dxlog_file,
          L: 192,
          S: this,
          C: (f, a) => f(...a)
        });
        continue;
      }
      const requestBitfield = header.bitfield ?? import_util.BitField.ones(meta.length / meta.chunkSize);
      const presentData = import_util.BitField.and(requestBitfield, meta.bitfield);
      const chunkIndices = import_util.BitField.findIndexes(presentData).sort(() => Math.random() - 0.5);
      for (const idx of chunkIndices) {
        const chunkData = await this._params.blobStore.get(header.id, {
          offset: idx * meta.chunkSize,
          length: Math.min(meta.chunkSize, meta.length - idx * meta.chunkSize)
        });
        chunks.push({
          id: header.id,
          totalLength: meta.length,
          chunkSize: meta.chunkSize,
          chunkOffset: idx * meta.chunkSize,
          payload: chunkData
        });
        if (chunks.length >= amount) {
          return chunks;
        }
      }
    }
    return chunks;
  }
};
_ts_decorate([
  import_async.synchronized
], BlobSyncExtension.prototype, "push", null);
function _ts_decorate2(decorators, target, key, desc) {
  var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;
  if (typeof Reflect === "object" && typeof Reflect.decorate === "function") r = Reflect.decorate(decorators, target, key, desc);
  else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;
  return c > 3 && r && Object.defineProperty(target, key, r), r;
}
var __dxlog_file2 = "/home/runner/work/dxos/dxos/packages/core/mesh/teleport-extension-object-sync/src/blob-sync.ts";
var BlobSync = class {
  constructor(_params) {
    this._params = _params;
    this._ctx = new import_context2.Context(void 0, {
      F: __dxlog_file2,
      L: 30
    });
    this._mutex = new import_async2.Mutex();
    this._downloadRequests = new import_util2.ComplexMap((key) => import_keys.PublicKey.from(key).toHex());
    this._extensions = /* @__PURE__ */ new Set();
  }
  async open() {
  }
  async close() {
    await this._ctx.dispose();
  }
  /**
  * Resolves when the object with the given id is fully downloaded in the blob store.
  *
  * @param id hex-encoded id of the object to download.
  */
  async download(ctx, id) {
    (0, import_log2.log)("download", {
      id
    }, {
      F: __dxlog_file2,
      L: 53,
      S: this,
      C: (f, a) => f(...a)
    });
    const request = await this._mutex.executeSynchronized(async () => {
      const existingRequest = this._downloadRequests.get(id);
      if (existingRequest) {
        existingRequest.counter++;
        return existingRequest;
      }
      const meta = await this._params.blobStore.getMeta(id);
      const request2 = {
        trigger: new import_async2.Trigger(),
        counter: 1,
        want: {
          id,
          chunkSize: meta?.chunkSize,
          bitfield: meta?.bitfield && Uint8Array.from(import_util2.BitField.invert(meta.bitfield))
        }
      };
      if (meta?.state === import_blob.BlobMeta.State.FULLY_PRESENT) {
        request2.trigger.wake();
      } else {
        this._downloadRequests.set(id, request2);
        this._updateExtensionsWantList();
      }
      return request2;
    });
    ctx?.onDispose(() => this._mutex.executeSynchronized(async () => {
      const request2 = this._downloadRequests.get(id);
      if (!request2) {
        return;
      }
      if (--request2.counter === 0) {
        this._downloadRequests.delete(id);
      }
      this._updateExtensionsWantList();
    }));
    return ctx ? (0, import_context2.cancelWithContext)(ctx, request.trigger.wait()) : request.trigger.wait();
  }
  createExtension() {
    const extension = new BlobSyncExtension({
      blobStore: this._params.blobStore,
      onOpen: async () => {
        (0, import_log2.log)("extension opened", void 0, {
          F: __dxlog_file2,
          L: 105,
          S: this,
          C: (f, a) => f(...a)
        });
        this._extensions.add(extension);
        extension.updateWantList(this._getWantList());
      },
      onClose: async () => {
        (0, import_log2.log)("extension closed", void 0, {
          F: __dxlog_file2,
          L: 110,
          S: this,
          C: (f, a) => f(...a)
        });
        this._extensions.delete(extension);
      },
      onAbort: async () => {
        (0, import_log2.log)("extension aborted", void 0, {
          F: __dxlog_file2,
          L: 114,
          S: this,
          C: (f, a) => f(...a)
        });
        this._extensions.delete(extension);
      },
      onPush: async (blobChunk) => {
        if (!this._downloadRequests.has(blobChunk.id)) {
          return;
        }
        (0, import_log2.log)("received", {
          blobChunk
        }, {
          F: __dxlog_file2,
          L: 121,
          S: this,
          C: (f, a) => f(...a)
        });
        const meta = await this._params.blobStore.setChunk(blobChunk);
        if (meta.state === import_blob.BlobMeta.State.FULLY_PRESENT) {
          this._downloadRequests.get(blobChunk.id)?.trigger.wake();
          this._downloadRequests.delete(blobChunk.id);
        } else {
          (0, import_invariant2.invariant)(meta.bitfield, void 0, {
            F: __dxlog_file2,
            L: 127,
            S: this,
            A: [
              "meta.bitfield",
              ""
            ]
          });
          this._downloadRequests.get(blobChunk.id).want.bitfield = import_util2.BitField.invert(meta.bitfield);
        }
        this._updateExtensionsWantList();
        this._reconcileUploads();
      }
    });
    return extension;
  }
  /**
  * Notify extensions that a blob with the given id was added to the blob store.
  */
  async notifyBlobAdded(_id) {
    this._reconcileUploads();
  }
  _getWantList() {
    return {
      blobs: Array.from(this._downloadRequests.values()).map((request) => request.want)
    };
  }
  _reconcileUploads() {
    for (const extension of this._extensions) {
      extension.reconcileUploads();
    }
  }
  _updateExtensionsWantList() {
    for (const extension of this._extensions) {
      extension.updateWantList(this._getWantList());
    }
  }
};
BlobSync = _ts_decorate2([
  (0, import_async2.trackLeaks)("open", "close")
], BlobSync);
function _ts_decorate3(decorators, target, key, desc) {
  var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;
  if (typeof Reflect === "object" && typeof Reflect.decorate === "function") r = Reflect.decorate(decorators, target, key, desc);
  else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;
  return c > 3 && r && Object.defineProperty(target, key, r), r;
}
var __dxlog_file3 = "/home/runner/work/dxos/dxos/packages/core/mesh/teleport-extension-object-sync/src/blob-store.ts";
var DEFAULT_CHUNK_SIZE = 4096;
var BlobMetaCodec = import_proto2.schema.getCodecForType("dxos.echo.blob.BlobMeta");
var BlobStore = class {
  constructor(_directory) {
    this._directory = _directory;
  }
  async getMeta(id) {
    return this._getMeta(id);
  }
  /**
  * @throws If range is not available.
  */
  async get(id, options = {}) {
    const metadata = await this._getMeta(id);
    if (!metadata) {
      throw new Error("Blob not available");
    }
    const { offset = 0, length = metadata.length } = options;
    if (offset + length > metadata.length) {
      throw new Error("Invalid range");
    }
    if (metadata.state === import_blob2.BlobMeta.State.FULLY_PRESENT) {
      const file2 = this._getDataFile(id);
      return file2.read(offset, length);
    } else if (options.offset === void 0 && options.length === void 0) {
      throw new Error("Blob not available");
    }
    const beginChunk = Math.floor(offset / metadata.chunkSize);
    const endChunk = Math.ceil((offset + length) / metadata.chunkSize);
    (0, import_invariant3.invariant)(metadata.bitfield, "Bitfield not present", {
      F: __dxlog_file3,
      L: 61,
      S: this,
      A: [
        "metadata.bitfield",
        "'Bitfield not present'"
      ]
    });
    (0, import_invariant3.invariant)(metadata.bitfield.length * 8 >= endChunk, "Invalid bitfield length", {
      F: __dxlog_file3,
      L: 62,
      S: this,
      A: [
        "metadata.bitfield.length * 8 >= endChunk",
        "'Invalid bitfield length'"
      ]
    });
    const present = import_util3.BitField.count(metadata.bitfield, beginChunk, endChunk) === endChunk - beginChunk;
    if (!present) {
      throw new Error("Blob not available");
    }
    const file = this._getDataFile(id);
    return file.read(offset, length);
  }
  async list() {
    const files = new Set((await this._directory.list()).map((f) => f.split("_")[0]));
    const res = [];
    for (const file of files) {
      const id = import_keys2.PublicKey.from(file).asUint8Array();
      const meta = await this._getMeta(id);
      if (meta) {
        res.push(meta);
      }
    }
    return res;
  }
  async set(data) {
    const id = new Uint8Array(await import_crypto.subtleCrypto.digest("SHA-256", data));
    const bitfield = import_util3.BitField.ones(data.length / DEFAULT_CHUNK_SIZE);
    const meta = {
      id,
      state: import_blob2.BlobMeta.State.FULLY_PRESENT,
      length: data.length,
      chunkSize: DEFAULT_CHUNK_SIZE,
      bitfield,
      created: /* @__PURE__ */ new Date(),
      updated: /* @__PURE__ */ new Date()
    };
    await this._getDataFile(id).write(0, (0, import_util3.arrayToBuffer)(data));
    await this._writeMeta(id, meta);
    return meta;
  }
  // TODO(dmaretskyi): Optimize locking.
  async setChunk(chunk) {
    let meta = await this._getMeta(chunk.id);
    if (!meta) {
      (0, import_invariant3.invariant)(chunk.totalLength, "totalLength is not present", {
        F: __dxlog_file3,
        L: 124,
        S: this,
        A: [
          "chunk.totalLength",
          "'totalLength is not present'"
        ]
      });
      meta = {
        id: chunk.id,
        state: import_blob2.BlobMeta.State.PARTIALLY_PRESENT,
        length: chunk.totalLength,
        chunkSize: chunk.chunkSize ?? DEFAULT_CHUNK_SIZE,
        created: /* @__PURE__ */ new Date()
      };
      meta.bitfield = import_util3.BitField.zeros(meta.length / meta.chunkSize);
    }
    if (chunk.chunkSize && chunk.chunkSize !== meta.chunkSize) {
      throw new Error("Invalid chunk size");
    }
    (0, import_invariant3.invariant)(meta.bitfield, "Bitfield not present", {
      F: __dxlog_file3,
      L: 139,
      S: this,
      A: [
        "meta.bitfield",
        "'Bitfield not present'"
      ]
    });
    (0, import_invariant3.invariant)(chunk.chunkOffset !== void 0, "chunkOffset is not present", {
      F: __dxlog_file3,
      L: 140,
      S: this,
      A: [
        "chunk.chunkOffset !== undefined",
        "'chunkOffset is not present'"
      ]
    });
    await this._getDataFile(chunk.id).write(chunk.chunkOffset, (0, import_util3.arrayToBuffer)(chunk.payload));
    import_util3.BitField.set(meta.bitfield, Math.floor(chunk.chunkOffset / meta.chunkSize), true);
    if (import_util3.BitField.count(meta.bitfield, 0, meta.length) * meta.chunkSize >= meta.length) {
      meta.state = import_blob2.BlobMeta.State.FULLY_PRESENT;
    }
    meta.updated = /* @__PURE__ */ new Date();
    await this._writeMeta(chunk.id, meta);
    return meta;
  }
  async _writeMeta(id, meta) {
    const encoded = (0, import_util3.arrayToBuffer)(BlobMetaCodec.encode(meta));
    const data = Buffer.alloc(encoded.length + 4);
    data.writeUInt32LE(encoded.length, 0);
    encoded.copy(data, 4);
    await this._getMetaFile(id).write(0, data);
  }
  async _getMeta(id) {
    const file = this._getMetaFile(id);
    const size = (await file.stat()).size;
    if (size === 0) {
      return;
    }
    const data = await file.read(0, size);
    const protoSize = data.readUInt32LE(0);
    return BlobMetaCodec.decode(data.subarray(4, protoSize + 4));
  }
  _getMetaFile(id) {
    return this._directory.getOrCreateFile(import_node_path.default.join((0, import_util3.arrayToBuffer)(id).toString("hex"), "meta"));
  }
  _getDataFile(id) {
    return this._directory.getOrCreateFile(import_node_path.default.join((0, import_util3.arrayToBuffer)(id).toString("hex"), "data"));
  }
};
_ts_decorate3([
  import_async3.synchronized
], BlobStore.prototype, "getMeta", null);
_ts_decorate3([
  import_async3.synchronized
], BlobStore.prototype, "get", null);
_ts_decorate3([
  import_async3.synchronized
], BlobStore.prototype, "list", null);
_ts_decorate3([
  import_async3.synchronized
], BlobStore.prototype, "set", null);
_ts_decorate3([
  import_async3.synchronized
], BlobStore.prototype, "setChunk", null);
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  BlobStore,
  BlobSync,
  BlobSyncExtension,
  DEFAULT_CHUNK_SIZE
});
//# sourceMappingURL=index.cjs.map
